Port turnaround model - Milestones

Step 1. Dataset assembly (complete – waiting on upstream refresh)

 Exported port_calls, voyages, vessels, cargo, commodity from DB (cutoff 31-12-2024).

 Wrote Models/build_port_turnaround_dataset.py to join and clean.

 Current data-cleaning decisions (reviewed with Codex):
  - Columns are upper-cased on load to simplify joins across heterogeneous extracts.
  - Mandatory numeric fields coerced with `pd.to_numeric(..., errors="coerce")` to guard against stray string values while preserving nulls for later auditing.
  - `DAYS_IN_PORT` bounded to [0.04, 10] days to remove implausible stays (<1 h or >10 d) before modelling.
  - Within-group trimming (`quantile(0.05)`-`0.95`) over (PORT_ID, TERMINAL_ID, IS_BALLAST) to cap extreme durations that would distort medians.
  - Missing `VESSEL_TYPE_ID` values backfilled from the vessel reference table; optional cargo/commodity tables treated as empty when absent so the pipeline still runs.

Environment notes:
  - README.md and TextFiles/runGuide.txt (2025-10-05) document uv-based setup for macOS/Linux and Windows (winget install, interpreter paths, PowerShell execution policy guidance).
  - All dataset/QA scripts accept `--customer northernlights|stena` (default Northern Lights) to switch raw/derived folders.

 QA and exploration plan for `port_turnaround_training`:

[x] 1. File availability and freshness
  - `Models/build_port_turnaround_dataset.py` rerun via `.venv/bin/python` (parquet write now optional; install `pyarrow` to re-enable).
  - `DataSets/Derived/port_turnaround_training.csv` refreshed (80 rows). Parquet skipped by design when engine missing.

[x] 2. Schema and type spot-check
  - `QA/port_turnaround_dataset_qa.py` writes `DataSets/Derived/NorthernLights/QA/reports/port_turnaround_training_overview.txt` with dtypes and null ratios (TERMINAL_ID 98.75% missing; VESSEL_TYPE_ID 13.75%; COMMODITY_ID 22.5%).
  - Figures saved to `DataSets/Derived/NorthernLights/QA/figures/port_turnaround_days_in_port_hist.svg` and `.../port_turnaround_missingness.svg` for thesis-ready visuals.

[x] 3. DAYS_IN_PORT distribution guardrails (2025-10-05 run via `uv run python QA/port_turnaround_dataset_qa.py`)
  - `DataSets/Derived/NorthernLights/QA/reports/port_turnaround_training_guardrails.txt` reports min 0.2000, median 1.2083, max 5.9785.
  - Guardrail breaches: 0 below 0.04 days, 0 above 10.0 days. No follow-up required.

[x] 4. Missingness audit on key fields
  - Re-run `uv run python QA/port_turnaround_dataset_qa.py` to regenerate:
    * `DataSets/Derived/NorthernLights/QA/tables/port_turnaround_missingness_by_port.csv`
    * `DataSets/Derived/NorthernLights/QA/tables/port_turnaround_missingness_by_port_terminal.csv`
    * `DataSets/Derived/NorthernLights/QA/reports/port_turnaround_missingness_notes.txt`
  - Review the `_missing_ratio` columns and capture hotspots (>50% missing).
  - 2025-10-05 findings: TERMINAL_ID is entirely missing for ports 104084 (n=33) and 110965 (n=30); several single-observation ports also lack VESSEL_TYPE_ID/COMMODITY_ID. Upstream extracts need terminal joins and complete vessel metadata; IS_BALLAST now OK.
[x] 5. Group coverage review
  - Review `DataSets/Derived/NorthernLights/QA/tables/port_turnaround_group_counts.csv` (generated by `uv run python QA/port_turnaround_dataset_qa.py`).
  - Flag groups with fewer than 5 observations and plan fallbacks.
  - 2025-10-05 findings: 16 groups fall below 5 observations (all but PORT_ID 101620 have null TERMINAL_ID); handle them via higher-level lookup tiers or request additional data.
[x] 6. Follow-up log
  - Document any cleanup tasks uncovered (e.g., columns to drop, imputation strategies) and feed them into the preprocessing backlog before moving to Step 2.
  - Upstream data fix: re-export port calls with terminal joins for ports 104084 and 110965.
  - Collect complete vessel metadata for small-port voyages so VESSEL_TYPE_ID backfills succeed; otherwise exclude until resupplied.
  - After data refresh, rerun dataset build + QA to validate missingness drops and revisit lookup fallbacks.

Step 2. Lookup model

 Create Models/build_port_turnaround_lookup.py.

 From port_turnaround_training.csv, compute robust medians with fallback hierarchy:
  (PORT, TERMINAL, IS_BALLAST, VESSEL_TYPE_ID, MONTH)
  (PORT, TERMINAL, IS_BALLAST, VESSEL_TYPE_ID)
  (PORT, TERMINAL, IS_BALLAST)
  (PORT, IS_BALLAST)
  (PORT)
  Global median.

 Apply min support (>=3 obs).
 Clamp results to [0.04, 10] days.
 Save to DataSets/Derived/port_turnaround_lookup.(csv|parquet).

Step 3. Validation

 Plot histogram of actual DAYS_IN_PORT vs lookup medians.
 Spot-check high-traffic ports (e.g. top 5 by obs).
 Check fallbacks are used correctly (groups with <3 obs fallback up).

Step 4. Ready for Optimizer

 Write helper function get_turnaround_time(port_id, terminal_id, is_ballast, vessel_type_id, month_no) that:
  - Looks up the most specific row.
  - Falls back progressively.
 Integrate this function in the optimizer preprocessing step.

Deliverables after completion
  - port_turnaround_training.csv (clean dataset for ML/QA).
  - port_turnaround_lookup.csv (ready for deterministic solver).
  - get_turnaround_time() function.
  - Validation plots and QA counts.

---

Port turnaround ML model - Milestones

Step 1. Feature engineering dataset

 - Build modelling view from `DataSets/Derived/<customer>/port_turnaround_training.csv`.
 - Core categorical features: PORT_ID, TERMINAL_ID, IS_BALLAST, VESSEL_TYPE_ID, MONTH_NO, HAS_CANAL_PASSAGE, COMMODITY_GROUP_ID/COMMODITY_CODE.
 - Numerical features: vessel dimensions (DWT_SUMMER, DRAFT_SUMMER, LOA, BEAM), DAYS_STOPPAGES, DAYS_EXTRA_IN_PORT.
 - Derived signals: seasonal dummies, interactions (e.g., PORT × IS_BALLAST), log-transform of DAYS_IN_PORT for variance control (track inverse transform).
 - Validation split: time-based (train ≤2024, validate 2025) plus optional grouped KFold (group by VOYAGE_ID or PORT_ID) if data volume allows.

Step 2. Baseline & candidate models

 - Baselines: lookup median (control), regularised linear models (Ridge/Lasso/ElasticNet), tree-based models (RandomForest, LightGBM/XGBoost).
 - Implement sklearn pipelines with categorical encoders (OneHot or TargetEncoder) and numeric scaling; manage high-cardinality ports/terminals with smoothing.
 - Optimisation metric: MAE (primary) with RMSE and SMAPE for reporting.

Step 3. Hyperparameter tuning & diagnostics

 - Use Optuna or HalvingGridSearchCV for efficient tuning; persist trial metadata.
 - Produce feature importance / SHAP insights and residual diagnostics segmented by PORT_ID, IS_BALLAST, VESSEL_TYPE_ID.
 - Persist best estimator + preprocessing artefacts under `Models/Artifacts/<customer>/port_turnaround_ml.joblib` (store metadata JSON).

Step 4. Packaging & documentation

 - Expose helper `predict_days_in_port(...)` wrapping preprocessing + model.
 - Add CLI entry `uv run python Models/train_port_turnaround_model.py --customer <slug>` and document in run guide.
 - Emit QA report to `DataSets/Derived/<customer>/QA/ml/port_turnaround_model_report.txt` summarising metrics vs lookup baseline.
 - Record outcomes (metrics, chosen model) in progress log + context helper after training.

Deliverables after completion
  - Trained model artefact + preprocessing bundle (`Models/Artifacts/<customer>/port_turnaround_ml.joblib`).
  - QA report with validation metrics/residual diagnostics.
  - Helper API for downstream optimiser integration.

Status (2025-10-16)
  - DecisionTreeRegressor baseline (max_depth=8, min_samples_leaf=20) with port-level median features achieves MAE ≈ 0.73 d on 2025 validation (lookup median baseline: 1.31 d).
  - HistGradientBoostingRegressor prototype (lr=0.03, log-target) currently trails (MAE ≈ 0.83 d); kept for future tuning but not preferred for production yet.
  - QA artefacts: `DataSets/Derived/<customer>/QA/ml/port_turnaround_model_report.txt` plus scatter/residual plots (validation and train splits).
  - Serving plan: expose `predict_days_in_port(...)` that loads artefacts, applies the tree pipeline, falls back to lookup when inputs are sparse.

---

Sailing Time Model - Milestones

Step 1. Dataset assembly

 Exported voyages_completed_asof_2025-12-31.csv already available.

 Write Models/build_sailing_time_dataset.py to:
  - Select relevant voyage fields: VOYAGE_ID, VESSEL_ID, VESSEL_TYPE_ID, MILES_BALLAST, MILES_LOADED, MILES_TOTAL, DAYS_TOTAL_AT_SEA, HAS_CANAL_PASSAGE, ESTIMATED_VOYAGE_START_DATE.
  - Join with vessels_reference.csv for size features (DWT, DRAFT, LOA, BEAM) and optional cargo counts per voyage.
  - Compute effective hours per nautical mile: `HOURS_PER_NM = (DAYS_AT_SEA * 24) / MILES_TOTAL`.
  - Engineer features: `BALLAST_FRAC = miles_ballast / miles_total`, `MONTH_NO`, voyage year, vessel age, optional voyage-level cargo metrics.
  - Save cleaned dataset as `DataSets/Derived/<customer>/sailing_time/sailing_time_training.(csv|parquet)`.
  - Guardrails: retain rows with HOURS_PER_NM in [0.02, 5.0]; drop vessels with names containing "test".

Step 2. Lookup model

 Create Models/build_sailing_time_lookup.py.

 From sailing_time_training.csv, compute robust medians of hours_per_nm with fallback hierarchy:
  (VESSEL_TYPE_ID, HAS_CANAL_PASSAGE, MONTH_NO)
  (VESSEL_TYPE_ID, HAS_CANAL_PASSAGE)
  (VESSEL_TYPE_ID)
  Global.

 Apply min support (>=3 obs).
 Clamp results to [0.02, 5] hours/nm (sane bounds).
 Save to DataSets/Derived/sailing_time_lookup.(csv|parquet).

Step 3. Validation

 Plot histogram of actual hours_per_nm vs lookup medians.
 Spot-check vessel types (e.g. MR vs Panamax).
 Check canal vs non-canal differences are reasonable.
 Verify seasonal (MONTH_NO) medians only if enough data.

Step 4. Ready for Optimizer

 Write helper function get_sailing_time(distance_nm, vessel_type_id, has_canal_passage, month_no) that:
  - Finds the most specific median hours_per_nm.
  - Falls back if group too small.
  - Returns `distance_nm * hours_per_nm`.
 Integrate into solver preprocessing to compute sailing times for candidate routes.

Status (2025-10-16)
  - Stena dataset built (`DataSets/Derived/Stena/sailing_time/sailing_time_training.csv`, n ≈ 1.2k) with QA counts under `/QA/`.
  - QA automation (`QA/sailing_time_dataset_qa.py`) and lookup generation (`Models/sailing_time_lookup.py`) complete; lookup stored at `DataSets/Derived/<customer>/sailing_time/sailing_time_lookup.(csv|parquet)` with QA bins.
  - Research-informed roadmap: start with tabular boosting (LightGBM/XGBoost) on current features, report MAE/sMAPE + residual percentiles; plan metocean enrichment (ERA5, WW3, CMEMS/OSCAR) for Phase 2, and consider grey-box residuals for physics robustness.
  - Splitting strategy will mirror port turnaround (time cutoff or `--validation-ratio`), with grouped/rolling splits for robustness documented in thesis.

 Deliverables after completion
  - sailing_time_training.csv (clean dataset for QA/ML).
  - sailing_time_lookup.csv (ready for solver).
  - get_sailing_time() function.
  - Validation plots and QA counts.


Meeting Notes – 2025-10-05

Current progress
- Step 1 dataset assembly is locked in: the build script trims 5% tails per (PORT_ID, TERMINAL_ID, IS_BALLAST), enforces 0.04–10 day guardrails, backfills vessel types, and ships both CSV and optional parquet via uv-friendly tooling.
- QA suite (`QA/port_turnaround_dataset_qa.py`) now generates overview/guardrail/missingness/coverage artefacts plus SVG charts that drop straight into the thesis pack.
- Lookup prototype (`Models/port_turnaround_lookup.py`) consumes the derived dataset, applies the fallback hierarchy (L1–L6), clamps medians, and records specificity so solver integration can pick the best match.

Key findings & rationale
- Guardrail stats: 80 usable port calls, min 0.20 d, median 1.21 d, max 5.98 d, no breaches outside the [0.04,10] window, confirming the trimming strategy keeps real operations intact.
- Missingness hotspots: ports 104084 (n=33) and 110965 (n=30) lack terminal IDs entirely; single-observation ports are the main source of VESSEL_TYPE_ID and COMMODITY gaps. These drive reliance on coarse lookup tiers.
- Coverage audit: only two groups exceed the ≥3 observation threshold at the most specific level; 16 groups fall below 5 obs, justifying the fallback ladder captured in the lookup file.
- Lookup QA bins show duration mass concentrated in 0.5–2 day buckets, aligning with the narrative of quick turnarounds for this pilot slice.

Dependencies & requests
- Need refreshed extracts with terminal joins for ports 104084/110965 and richer vessel metadata for sparse ports to unlock more specific lookup rows.
- After the data refresh, rerun `Models/build_port_turnaround_dataset.py` and both QA scripts to validate reduced missingness before moving to Step 2 automation.

Planned next steps
- Step 2: finalise lookup construction (post-refresh) and add unit-style checks ensuring fallback selection logic aligns with specificity scores.
- Step 3: validation plots contrasting lookup medians vs observed durations, plus narrative on fallback utilisation.
- Step 4: expose `get_turnaround_time(...)` helper and integrate with the optimisation pre-processor once lookup fidelity is acceptable.

Talking points for advisor meeting
- Emphasise reproducibility: uv-managed environment, run guide, and automated QA outputs make the pipeline shareable across platforms.
- Highlight current data limitations (terminal IDs + vessel metadata) and the concrete upstream actions requested to unblock finer segmentation.
- Outline how the lookup’s fallback structure will evolve with richer data and how it will feed the optimiser in later phases.
- Flag that Step 1 documentation (README, context helper, run guide) now captures Windows + macOS workflows, supporting industry collaboration later in the project.
